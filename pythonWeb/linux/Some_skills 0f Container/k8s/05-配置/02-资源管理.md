# 资源管理

> requests 和 limits、LimitRange、ResourceQuota

## 概述

【本文档是 Kubernetes 知识体系的一部分】

**资源管理是 Kubernetes 中确保集群资源合理分配和使用的关键机制**。通过 requests 和 limits,Kubernetes 可以为容器预留资源、限制资源使用,防止单个应用消耗过多资源影响集群稳定性。

## 核心概念

### 什么是资源管理

Kubernetes 支持两类资源:

| 资源类型 | 说明 | 单位 |
|---------|------|------|
| **CPU** | 可压缩资源,超限会被限流 | millicores (m) 或 cores |
| **内存** | 不可压缩资源,超限会被 OOM Kill | bytes (Ki, Mi, Gi) |
| **临时存储** | 本地临时存储(emptyDir、日志) | bytes (Ki, Mi, Gi) |
| **扩展资源** | GPU、FPGA 等自定义资源 | 整数 |

### requests vs limits

```
┌─────────────────────────────────────┐
│       资源使用示意图                 │
│                                     │
│  ┌────────────────────────────┐   │
│  │ limits (资源上限)           │   │
│  │    ▲                        │   │
│  │    │ 超过 limits            │   │
│  │    │ → CPU 限流             │   │
│  │    │ → 内存 OOMKilled       │   │
│  ├────┼────────────────────────┤   │
│  │    │                        │   │
│  │    │ 实际使用               │   │
│  │    │                        │   │
│  ├────┼────────────────────────┤   │
│  │ requests (资源预留)         │   │
│  │    │ 调度依据               │   │
│  │    ▼ QoS 分类依据           │   │
│  └────────────────────────────┘   │
└─────────────────────────────────────┘
```

| 配置项 | 作用 | 影响 |
|-------|------|------|
| **requests** | 资源预留量 | 调度器保证节点有足够资源 |
| **limits** | 资源上限 | CPU 限流、内存 OOM Kill |

### QoS 类别

Pod 的 QoS 类别由 requests 和 limits 决定:

| QoS 类别 | 条件 | 驱逐优先级 |
|---------|------|-----------|
| **Guaranteed** | requests == limits(所有容器) | 最低(最后被驱逐) |
| **Burstable** | 0 < requests < limits | 中等 |
| **BestEffort** | 未设置 requests 和 limits | 最高(最先被驱逐) |

#### QoS 判断逻辑

```go
// 简化的 QoS 判断伪代码
func GetQoS(pod Pod) QoSClass {
    requests := 0
    limits := 0

    for _, container := range pod.Spec.Containers {
        if container.Resources.Requests != nil {
            requests++
        }
        if container.Resources.Limits != nil {
            limits++
        }
    }

    if requests == limits && requests == len(pod.Spec.Containers) {
        // 所有容器都设置了 requests 和 limits,且相等
        return Guaranteed
    }

    if requests > 0 || limits > 0 {
        return Burstable
    }

    return BestEffort
}
```

---

## requests 和 limits 使用

### 1. 基础配置

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-demo
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "250m"       # 0.25 核 CPU
        memory: "64Mi"    # 64 MiB 内存
      limits:
        cpu: "500m"       # 最多使用 0.5 核 CPU
        memory: "128Mi"   # 最多使用 128 MiB 内存
```

#### CPU 单位说明

```bash
# CPU 单位
1 CPU = 1000m (millicores)
0.5 CPU = 500m
0.25 CPU = 250m

# 示例
requests.cpu: "1"      # 1 核 CPU
requests.cpu: "500m"   # 0.5 核 CPU
requests.cpu: "100m"   # 0.1 核 CPU
```

#### 内存单位说明

```bash
# 内存单位
1 Ki = 1024 bytes
1 Mi = 1024 Ki = 1048576 bytes
1 Gi = 1024 Mi = 1073741824 bytes

# 示例
memory: "128Mi"   # 128 MiB
memory: "1Gi"     # 1 GiB
memory: "512Mi"   # 512 MiB
```

### 2. Guaranteed QoS 示例

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: guaranteed-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "1"
        memory: "1Gi"
      limits:
        cpu: "1"        # requests == limits
        memory: "1Gi"   # requests == limits
```

**验证 QoS**:
```bash
kubectl get pod guaranteed-pod -o yaml | grep qosClass
# qosClass: Guaranteed
```

### 3. Burstable QoS 示例

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: burstable-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "250m"
        memory: "256Mi"
      limits:
        cpu: "1"        # limits > requests
        memory: "512Mi" # limits > requests
```

### 4. BestEffort QoS 示例

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: besteffort-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
    # 不设置 resources
```

### 5. 多容器配置

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "500m"
        memory: "512Mi"
      limits:
        cpu: "1"
        memory: "1Gi"
  - name: sidecar
    image: busybox:1.35
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "200m"
        memory: "256Mi"
  # Pod 总 requests: 600m CPU, 640Mi 内存
  # Pod 总 limits: 1200m CPU, 1280Mi 内存
```

### 6. 临时存储限制

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ephemeral-storage-pod
spec:
  containers:
  - name: app
    image: busybox:1.35
    command: ["/bin/sh", "-c", "dd if=/dev/zero of=/tmp/file bs=1M count=100"]
    resources:
      requests:
        ephemeral-storage: "1Gi"
      limits:
        ephemeral-storage: "2Gi"
```

---

## LimitRange

### 什么是 LimitRange

**LimitRange 用于限制 Namespace 中资源的使用范围**,可以设置:
- Pod/Container 的最小和最大资源
- 默认的 requests 和 limits
- requests 和 limits 的比例

### 1. Container 级别限制

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: container-limit-range
  namespace: default
spec:
  limits:
  - type: Container
    max:
      cpu: "2"
      memory: "2Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    default:
      cpu: "500m"      # 默认 limits
      memory: "512Mi"
    defaultRequest:
      cpu: "250m"      # 默认 requests
      memory: "256Mi"
    maxLimitRequestRatio:
      cpu: 4           # limits 最多是 requests 的 4 倍
      memory: 2        # limits 最多是 requests 的 2 倍
```

#### 应用后效果

```yaml
# 用户创建 Pod(未指定 resources)
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
  - name: app
    image: nginx:1.21

# Kubernetes 自动添加(来自 LimitRange)
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "250m"
        memory: "256Mi"
      limits:
        cpu: "500m"
        memory: "512Mi"
```

### 2. Pod 级别限制

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: pod-limit-range
  namespace: default
spec:
  limits:
  - type: Pod
    max:
      cpu: "4"
      memory: "4Gi"
    min:
      cpu: "200m"
      memory: "256Mi"
```

### 3. PVC 限制

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: storage-limit-range
  namespace: default
spec:
  limits:
  - type: PersistentVolumeClaim
    max:
      storage: "10Gi"
    min:
      storage: "1Gi"
```

### 4. 查看 LimitRange

```bash
# 查看 LimitRange
kubectl get limitrange -n default

# 详细信息
kubectl describe limitrange container-limit-range -n default

# 示例输出
Name:       container-limit-range
Namespace:  default
Type        Resource  Min    Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---    ---   ---------------  -------------  -----------------------
Container   cpu       100m   2     250m             500m           4
Container   memory    128Mi  2Gi   256Mi            512Mi          2
```

---

## ResourceQuota

### 什么是 ResourceQuota

**ResourceQuota 用于限制 Namespace 的总资源使用量**,防止单个 Namespace 消耗过多资源。

### 1. 计算资源配额

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    # CPU 和内存总量
    requests.cpu: "10"
    requests.memory: "20Gi"
    limits.cpu: "20"
    limits.memory: "40Gi"

    # Pod 数量限制
    pods: "50"
```

#### 验证配额

```bash
# 查看配额使用情况
kubectl get resourcequota compute-quota -n dev

# 示例输出
NAME            CREATED AT
compute-quota   2025-12-03T10:00:00Z

# 详细信息
kubectl describe resourcequota compute-quota -n dev

# 输出
Name:           compute-quota
Namespace:      dev
Resource        Used   Hard
--------        ----   ----
limits.cpu      5      20
limits.memory   10Gi   40Gi
pods            15     50
requests.cpu    2.5    10
requests.memory 5Gi    20Gi
```

### 2. 对象数量配额

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-quota
  namespace: dev
spec:
  hard:
    # 对象数量限制
    pods: "50"
    services: "10"
    configmaps: "20"
    secrets: "20"
    persistentvolumeclaims: "10"
    replicationcontrollers: "10"

    # Deployment/StatefulSet 等
    count/deployments.apps: "10"
    count/statefulsets.apps: "5"
    count/jobs.batch: "20"
```

### 3. 按优先级配额

```yaml
---
# 高优先级 PriorityClass
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000
globalDefault: false
description: "High priority class"

---
# 低优先级 PriorityClass
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 100
globalDefault: true
description: "Low priority class"

---
# 高优先级配额
apiVersion: v1
kind: ResourceQuota
metadata:
  name: high-priority-quota
  namespace: dev
spec:
  hard:
    requests.cpu: "20"
    requests.memory: "40Gi"
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["high-priority"]

---
# 低优先级配额
apiVersion: v1
kind: ResourceQuota
metadata:
  name: low-priority-quota
  namespace: dev
spec:
  hard:
    requests.cpu: "5"
    requests.memory: "10Gi"
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["low-priority"]
```

### 4. 存储配额

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-quota
  namespace: dev
spec:
  hard:
    # PVC 数量
    persistentvolumeclaims: "10"

    # 总存储大小
    requests.storage: "100Gi"

    # 按 StorageClass 限制
    requests.storage.storageclass.storage.k8s.io/fast: "50Gi"
    requests.storage.storageclass.storage.k8s.io/slow: "200Gi"
```

### 5. 按作用域配额

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: scoped-quota
  namespace: dev
spec:
  hard:
    requests.cpu: "5"
    requests.memory: "10Gi"
  scopes:
  - BestEffort     # 只对 BestEffort Pod 生效
  # 其他作用域:
  # - Terminating   (有 activeDeadlineSeconds 的 Pod)
  # - NotTerminating
  # - NotBestEffort (Guaranteed 和 Burstable)
```

---

## 实战案例

### 案例 1: 开发环境资源管理

```yaml
---
# Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: dev

---
# LimitRange(设置默认值和限制)
apiVersion: v1
kind: LimitRange
metadata:
  name: dev-limit-range
  namespace: dev
spec:
  limits:
  - type: Container
    max:
      cpu: "1"
      memory: "1Gi"
    min:
      cpu: "50m"
      memory: "64Mi"
    default:
      cpu: "200m"
      memory: "256Mi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
  - type: Pod
    max:
      cpu: "2"
      memory: "2Gi"

---
# ResourceQuota(总量限制)
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: dev
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "20Gi"
    limits.cpu: "20"
    limits.memory: "40Gi"
    pods: "50"
    services: "10"
    persistentvolumeclaims: "10"
```

**测试**:
```bash
# 应用配置
kubectl apply -f dev-resources.yaml

# 创建测试 Deployment
kubectl create deployment nginx --image=nginx:1.21 --replicas=3 -n dev

# 查看 Pod(会自动应用 LimitRange 的默认值)
kubectl get pod -n dev -o yaml | grep -A 6 resources:

# 查看配额使用
kubectl describe resourcequota dev-quota -n dev
```

### 案例 2: 生产环境多租户隔离

```yaml
---
# 租户 A
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-a

---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-a-quota
  namespace: tenant-a
spec:
  hard:
    requests.cpu: "50"
    requests.memory: "100Gi"
    limits.cpu: "100"
    limits.memory: "200Gi"
    pods: "200"
    services.loadbalancers: "5"

---
# 租户 B
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-b

---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-b-quota
  namespace: tenant-b
spec:
  hard:
    requests.cpu: "30"
    requests.memory: "60Gi"
    limits.cpu: "60"
    limits.memory: "120Gi"
    pods: "100"
```

### 案例 3: GPU 资源管理

```yaml
---
# 节点打标签
# kubectl label nodes gpu-node-1 accelerator=nvidia-tesla-v100

---
# Pod 请求 GPU
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  containers:
  - name: tensorflow
    image: tensorflow/tensorflow:latest-gpu
    resources:
      requests:
        cpu: "4"
        memory: "8Gi"
        nvidia.com/gpu: 1  # 请求 1 个 GPU
      limits:
        cpu: "8"
        memory: "16Gi"
        nvidia.com/gpu: 1
  nodeSelector:
    accelerator: nvidia-tesla-v100

---
# GPU ResourceQuota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: gpu-quota
  namespace: ml-team
spec:
  hard:
    requests.nvidia.com/gpu: "4"  # 最多 4 个 GPU
    limits.nvidia.com/gpu: "4"
```

---

## 资源驱逐

### 节点压力驱逐

当节点资源不足时,kubelet 会驱逐 Pod:

```yaml
# kubelet 配置
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
evictionHard:
  memory.available: "100Mi"
  nodefs.available: "10%"
  nodefs.inodesFree: "5%"
  imagefs.available: "15%"
evictionSoft:
  memory.available: "200Mi"
  nodefs.available: "15%"
evictionSoftGracePeriod:
  memory.available: "1m30s"
  nodefs.available: "2m"
evictionMaxPodGracePeriod: 60
```

### 驱逐顺序

1. **BestEffort Pod**(未设置 requests/limits)
2. **Burstable Pod 超过 requests**
3. **Burstable Pod 未超过 requests**
4. **Guaranteed Pod**(最后)

### 驱逐信号

| 信号 | 说明 | 驱逐行为 |
|-----|------|---------|
| **memory.available** | 可用内存 | 驱逐 Pod |
| **nodefs.available** | 节点文件系统可用空间 | 驱逐 Pod |
| **nodefs.inodesFree** | 节点文件系统可用 inode | 驱逐 Pod |
| **imagefs.available** | 镜像文件系统可用空间 | 删除未使用镜像 |

---

## 最佳实践

### 1. 始终设置 requests 和 limits

```yaml
# ✅ 好的做法
apiVersion: v1
kind: Pod
metadata:
  name: good-practice
spec:
  containers:
  - name: app
    image: myapp:1.0
    resources:
      requests:
        cpu: "250m"
        memory: "256Mi"
      limits:
        cpu: "500m"
        memory: "512Mi"

# ❌ 不推荐
apiVersion: v1
kind: Pod
metadata:
  name: bad-practice
spec:
  containers:
  - name: app
    image: myapp:1.0
    # 没有设置 resources
```

### 2. 生产环境使用 Guaranteed QoS

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: production-app
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: app
        image: myapp:1.0
        resources:
          requests:
            cpu: "1"
            memory: "1Gi"
          limits:
            cpu: "1"        # 与 requests 相同
            memory: "1Gi"   # 与 requests 相同
```

### 3. 合理设置 requests 和 limits 比例

```yaml
# CPU 可压缩,limits 可以大于 requests
resources:
  requests:
    cpu: "500m"
  limits:
    cpu: "2"    # 允许突发到 2 核

# 内存不可压缩,limits 不要过大
resources:
  requests:
    memory: "512Mi"
  limits:
    memory: "1Gi"  # 最多 2 倍
```

### 4. 使用 LimitRange 设置默认值

```yaml
# 为 Namespace 设置默认值
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limit-range
  namespace: production
spec:
  limits:
  - type: Container
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "250m"
      memory: "256Mi"
```

### 5. 使用 ResourceQuota 防止资源耗尽

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    requests.cpu: "100"
    requests.memory: "200Gi"
    limits.cpu: "200"
    limits.memory: "400Gi"
    pods: "500"
```

### 6. 监控资源使用

```bash
# 查看节点资源使用
kubectl top nodes

# 查看 Pod 资源使用
kubectl top pods -n production

# 查看配额使用情况
kubectl describe resourcequota -n production

# Prometheus 查询
# - container_memory_usage_bytes
# - container_cpu_usage_seconds_total
# - kube_pod_container_resource_requests
# - kube_pod_container_resource_limits
```

---

## 故障排查

### 问题 1: Pod 无法调度

```bash
# 症状
kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
myapp   0/1     Pending   0          5m

# 排查
kubectl describe pod myapp

# 可能原因
Events:
  Warning  FailedScheduling  5m  default-scheduler  0/3 nodes are available:
  3 Insufficient cpu.

# 解决方案
# 1. 降低 requests
# 2. 增加节点
# 3. 检查 ResourceQuota
kubectl describe resourcequota -n default
```

### 问题 2: OOMKilled

```bash
# 症状
kubectl get pods
NAME    READY   STATUS      RESTARTS   AGE
myapp   0/1     OOMKilled   5          10m

# 排查
kubectl describe pod myapp

# 原因
Last State:     Terminated
  Reason:       OOMKilled
  Exit Code:    137

# 解决方案
# 增加 memory limits
resources:
  limits:
    memory: "1Gi"  # 原来是 512Mi
```

### 问题 3: CPU 限流

```bash
# 症状: 应用响应慢

# 排查
kubectl top pod myapp
NAME    CPU(cores)   MEMORY(bytes)
myapp   500m         256Mi

# 查看配置
kubectl get pod myapp -o yaml | grep -A 6 resources:
resources:
  limits:
    cpu: 500m  # 已达到上限

# 解决方案
# 增加 CPU limits 或 优化应用性能
```

### 问题 4: ResourceQuota 超限

```bash
# 症状
Error from server (Forbidden): error when creating "deployment.yaml":
deployments.apps "nginx" is forbidden: exceeded quota: compute-quota,
requested: requests.cpu=1, used: requests.cpu=10, limited: requests.cpu=10

# 排查
kubectl describe resourcequota compute-quota

# 解决方案
# 1. 删除不用的资源
kubectl delete deployment old-app

# 2. 调整配额
kubectl edit resourcequota compute-quota

# 3. 降低新应用的 requests
```

---

## 常见问题

### Q1: requests 和 limits 的区别是什么?

**A**:
- **requests**: 调度依据,保证最少资源
- **limits**: 运行时限制,防止资源滥用

```yaml
resources:
  requests:
    cpu: "500m"    # 调度器保证至少 500m
    memory: "512Mi"
  limits:
    cpu: "1"       # 运行时最多使用 1 核
    memory: "1Gi"  # 运行时最多使用 1Gi
```

### Q2: CPU 和内存超限行为有什么不同?

**A**:
| 资源 | 超限行为 | 原因 |
|-----|---------|------|
| **CPU** | 限流(throttling) | 可压缩资源 |
| **内存** | OOMKilled | 不可压缩资源 |

### Q3: 如何选择合适的 requests 和 limits?

**A**:
```bash
# 1. 压测获取应用资源使用情况
kubectl top pod myapp --containers

# 2. 根据监控数据设置
requests:
  cpu: P50 使用量
  memory: P90 使用量

limits:
  cpu: P95 使用量
  memory: P99 使用量 + 20% 缓冲

# 3. 生产环境建议
# - 关键应用: Guaranteed QoS
# - 普通应用: Burstable QoS
# - 测试任务: BestEffort QoS
```

### Q4: LimitRange 和 ResourceQuota 有什么区别?

**A**:
| 特性 | LimitRange | ResourceQuota |
|-----|-----------|--------------|
| **作用域** | 单个 Pod/Container | 整个 Namespace |
| **功能** | 设置默认值、最小最大值 | 限制总量 |
| **示例** | 容器最多 2 核 CPU | Namespace 最多 100 核 CPU |

### Q5: 如何实现 GPU 资源管理?

**A**:
```yaml
# 1. 安装 GPU 设备插件
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/main/nvidia-device-plugin.yml

# 2. 在 Pod 中请求 GPU
resources:
  requests:
    nvidia.com/gpu: 1
  limits:
    nvidia.com/gpu: 1

# 3. 配置 ResourceQuota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: gpu-quota
spec:
  hard:
    requests.nvidia.com/gpu: "4"
```

---

## 总结

- ✅ **requests**: 调度依据,保证最小资源
- ✅ **limits**: 运行时限制,防止资源滥用
- ✅ **QoS 类别**: Guaranteed > Burstable > BestEffort
- ✅ **LimitRange**: 设置 Namespace 级别的默认值和限制
- ✅ **ResourceQuota**: 限制 Namespace 的总资源使用量
- ✅ **最佳实践**: 生产环境使用 Guaranteed,设置合理的 requests/limits 比例
- ✅ **监控**: 持续监控资源使用,及时调整配额

## 参考资源

- [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
- [Resource Quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/)
- [Limit Ranges](https://kubernetes.io/docs/concepts/policy/limit-range/)
- [Configure Quality of Service](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)
- [Node Pressure Eviction](https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/)

---

*更新日期: 2025-12-03*
