# 资源配额

> ResourceQuota、LimitRange、QoS 类、优先级与抢占

## 概述

【本文档是 Kubernetes 知识体系的一部分】

资源配额机制用于限制命名空间中的资源使用量,防止单个租户或应用占用过多集群资源。通过 ResourceQuota、LimitRange 和 QoS 类,可以实现精细的资源管理和调度优先级控制。

## 核心概念

### 什么是资源配额

**ResourceQuota (资源配额):**
- 限制命名空间中的总资源使用量
- 控制对象数量(Pod、Service、ConfigMap 等)
- 多租户环境的资源隔离

**LimitRange (限制范围):**
- 限制单个 Pod/容器的资源范围
- 设置默认资源请求和限制
- 防止创建过大或过小的 Pod

**QoS (服务质量)类:**
- Guaranteed: 最高优先级,资源有保证
- Burstable: 中等优先级,部分资源有保证
- BestEffort: 最低优先级,无资源保证

### 为什么需要

在生产环境中,我们需要资源配额来实现:

1. **成本控制**: 限制各团队的资源使用量
2. **公平共享**: 防止单个应用占用过多资源
3. **稳定性**: 避免资源耗尽导致集群不稳定
4. **多租户隔离**: 不同租户间资源隔离
5. **容量规划**: 根据配额进行集群容量规划

### 资源类型

**计算资源:**
```yaml
requests.cpu: "4"          # CPU 请求总量
requests.memory: "8Gi"     # 内存请求总量
limits.cpu: "8"            # CPU 限制总量
limits.memory: "16Gi"      # 内存限制总量
```

**存储资源:**
```yaml
requests.storage: "100Gi"           # 存储请求总量
persistentvolumeclaims: "10"        # PVC 数量
requests.ephemeral-storage: "10Gi"  # 临时存储
```

**对象数量:**
```yaml
count/pods: "50"                    # Pod 数量
count/services: "20"                # Service 数量
count/configmaps: "50"              # ConfigMap 数量
count/secrets: "50"                 # Secret 数量
count/persistentvolumeclaims: "10"  # PVC 数量
```

## ResourceQuota (资源配额)

### 基本使用

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: production
spec:
  hard:
    # 计算资源
    requests.cpu: "10"        # 总请求 10 核
    requests.memory: "20Gi"   # 总请求 20Gi 内存
    limits.cpu: "20"          # 总限制 20 核
    limits.memory: "40Gi"     # 总限制 40Gi 内存

    # 对象数量
    pods: "50"                # 最多 50 个 Pod
    services: "20"            # 最多 20 个 Service
    persistentvolumeclaims: "10"  # 最多 10 个 PVC
```

### 查看配额

```bash
# 查看 ResourceQuota
kubectl get resourcequota -n production

# 详细信息
kubectl describe resourcequota compute-quota -n production

# 输出示例:
# Name:                   compute-quota
# Namespace:              production
# Resource                Used   Hard
# --------                ----   ----
# limits.cpu              8      20
# limits.memory           16Gi   40Gi
# persistentvolumeclaims  3      10
# pods                    20     50
# requests.cpu            4      10
# requests.memory         8Gi    20Gi
# services                5      20
```

### 完整配额示例

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: namespace-quota
  namespace: team-a
spec:
  hard:
    # === 计算资源 ===
    requests.cpu: "20"
    requests.memory: "50Gi"
    limits.cpu: "40"
    limits.memory: "100Gi"

    # === 存储资源 ===
    requests.storage: "500Gi"
    persistentvolumeclaims: "20"
    requests.ephemeral-storage: "50Gi"
    limits.ephemeral-storage: "100Gi"

    # === GPU 资源 ===
    requests.nvidia.com/gpu: "4"

    # === 对象数量 ===
    count/pods: "100"
    count/services: "50"
    count/services.loadbalancers: "5"
    count/services.nodeports: "10"
    count/configmaps: "100"
    count/secrets: "100"
    count/replicationcontrollers: "20"
    count/deployments.apps: "30"
    count/statefulsets.apps: "10"
    count/jobs.batch: "20"
    count/cronjobs.batch: "10"

    # === 扩展资源 ===
    requests.hugepages-2Mi: "10Gi"
```

## LimitRange (限制范围)

### 基本使用

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-range
  namespace: production
spec:
  limits:
  # === Pod 限制 ===
  - type: Pod
    max:
      cpu: "4"
      memory: "8Gi"
    min:
      cpu: "100m"
      memory: "128Mi"

  # === 容器限制 ===
  - type: Container
    # 最大值
    max:
      cpu: "2"
      memory: "4Gi"
    # 最小值
    min:
      cpu: "50m"
      memory: "64Mi"
    # 默认值
    default:
      cpu: "500m"
      memory: "512Mi"
    # 默认请求
    defaultRequest:
      cpu: "250m"
      memory: "256Mi"
    # 最大请求/限制比例
    maxLimitRequestRatio:
      cpu: "2"      # limits.cpu / requests.cpu <= 2
      memory: "2"   # limits.memory / requests.memory <= 2

  # === PVC 限制 ===
  - type: PersistentVolumeClaim
    max:
      storage: "100Gi"
    min:
      storage: "1Gi"
```

### 自动注入默认值

创建 Pod 时如果没有指定 resources,会自动注入:

```yaml
# 创建的 Pod (没有 resources)
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
---
# 实际运行的 Pod (自动注入)
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: "250m"         # 来自 LimitRange
        memory: "256Mi"     # 来自 LimitRange
      limits:
        cpu: "500m"         # 来自 LimitRange
        memory: "512Mi"     # 来自 LimitRange
```

### 验证和拒绝

```bash
# 超过限制的 Pod 会被拒绝创建
kubectl run big-pod --image=nginx --limits=cpu=5,memory=10Gi -n production

# 错误信息:
# Error from server (Forbidden): pods "big-pod" is forbidden:
# maximum cpu usage per Container is 2, but limit is 5
```

## QoS 类(服务质量)

### Guaranteed (保证)

**条件:**
- 每个容器都设置了 requests 和 limits
- 每个容器的 requests.cpu == limits.cpu
- 每个容器的 requests.memory == limits.memory

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: guaranteed-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        memory: "512Mi"
        cpu: "500m"
      limits:
        memory: "512Mi"  # 等于 requests
        cpu: "500m"      # 等于 requests
```

**特点:**
- 最高优先级
- 资源有保证,不会被抢占
- OOM 时最后被 Kill
- 适合关键业务应用

### Burstable (可突发)

**条件:**
- 至少一个容器设置了 requests 或 limits
- 不满足 Guaranteed 条件

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: burstable-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        memory: "256Mi"
        cpu: "250m"
      limits:
        memory: "512Mi"  # 大于 requests
        cpu: "1000m"     # 大于 requests
```

**特点:**
- 中等优先级
- 有最小资源保证
- 可以使用超过 requests 的资源
- 资源不足时可能被驱逐
- 适合普通业务应用

### BestEffort (尽力而为)

**条件:**
- 所有容器都没有设置 requests 和 limits

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: besteffort-pod
spec:
  containers:
  - name: app
    image: nginx:1.21
    # 没有 resources 配置
```

**特点:**
- 最低优先级
- 无资源保证
- 可以使用空闲资源
- 资源不足时最先被驱逐
- 适合批处理、测试任务

### QoS 优先级

```
资源充足时:
┌──────────────────────┐
│  所有 Pod 正常运行    │
└──────────────────────┘

资源不足时:
┌──────────────────────┐
│  Guaranteed Pod      │  保留
├──────────────────────┤
│  Burstable Pod       │  可能被驱逐
├──────────────────────┤
│  BestEffort Pod      │  最先被驱逐
└──────────────────────┘
```

### 查看 QoS 类

```bash
# 查看 Pod 的 QoS 类
kubectl get pod my-pod -o jsonpath='{.status.qosClass}'

# 查看所有 Pod 的 QoS
kubectl get pods -o custom-columns=NAME:.metadata.name,QOS:.status.qosClass
```

## 优先级和抢占

### PriorityClass (优先级类)

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000          # 优先级值(越大越高)
globalDefault: false    # 是否为默认优先级
description: "关键业务高优先级"
```

**系统预定义优先级:**
```yaml
system-cluster-critical: 2000000000  # 系统组件
system-node-critical:    2000001000  # 节点组件
```

### 使用优先级

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: high-priority-pod
spec:
  priorityClassName: high-priority  # 使用优先级类

  containers:
  - name: app
    image: critical-app:latest
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1000m"
```

### 抢占机制

```
场景: 集群资源不足,无法调度高优先级 Pod

1. 调度器尝试调度高优先级 Pod
2. 发现资源不足
3. 查找低优先级 Pod 进行抢占
4. 驱逐低优先级 Pod
5. 调度高优先级 Pod
```

**示例:**
```yaml
# 高优先级 Pod
apiVersion: v1
kind: Pod
metadata:
  name: critical-pod
spec:
  priorityClassName: high-priority  # 优先级 1000000

  containers:
  - name: app
    image: critical-app:latest
    resources:
      requests:
        memory: "4Gi"
        cpu: "2000m"
---
# 低优先级 Pod (可能被抢占)
apiVersion: v1
kind: Pod
metadata:
  name: normal-pod
spec:
  priorityClassName: normal-priority  # 优先级 100

  containers:
  - name: app
    image: normal-app:latest
    resources:
      requests:
        memory: "2Gi"
        cpu: "1000m"
```

## 实战案例

### 案例 1: 多租户资源隔离

```yaml
# 租户 A 的命名空间和配额
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-a
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-a-quota
  namespace: tenant-a
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "20Gi"
    limits.cpu: "20"
    limits.memory: "40Gi"
    pods: "100"
    services: "50"
    persistentvolumeclaims: "20"
---
apiVersion: v1
kind: LimitRange
metadata:
  name: tenant-a-limits
  namespace: tenant-a
spec:
  limits:
  - type: Container
    max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "50m"
      memory: "64Mi"
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "250m"
      memory: "256Mi"
---
# 租户 B 的配额 (资源更少)
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-b
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-b-quota
  namespace: tenant-b
spec:
  hard:
    requests.cpu: "5"
    requests.memory: "10Gi"
    limits.cpu: "10"
    limits.memory: "20Gi"
    pods: "50"
```

### 案例 2: 环境分级配额

```yaml
# 生产环境 (资源充足)
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    env: production
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    requests.cpu: "50"
    requests.memory: "100Gi"
    limits.cpu: "100"
    limits.memory: "200Gi"
    pods: "200"
---
# 测试环境 (资源有限)
apiVersion: v1
kind: Namespace
metadata:
  name: staging
  labels:
    env: staging
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: staging-quota
  namespace: staging
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "20Gi"
    limits.cpu: "20"
    limits.memory: "40Gi"
    pods: "50"
---
# 开发环境 (最少资源)
apiVersion: v1
kind: Namespace
metadata:
  name: development
  labels:
    env: development
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: development-quota
  namespace: development
spec:
  hard:
    requests.cpu: "5"
    requests.memory: "10Gi"
    limits.cpu: "10"
    limits.memory: "20Gi"
    pods: "30"
```

### 案例 3: 关键应用优先级

```yaml
# 优先级类定义
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: critical
value: 1000000
globalDefault: false
description: "关键业务应用"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: normal
value: 100000
globalDefault: true
description: "普通业务应用"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low
value: 10000
globalDefault: false
description: "批处理任务"
---
# 关键应用 (Guaranteed + 高优先级)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: critical
  template:
    metadata:
      labels:
        app: critical
    spec:
      priorityClassName: critical

      containers:
      - name: app
        image: critical-app:latest
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "1Gi"    # Guaranteed QoS
            cpu: "500m"

        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
---
# 普通应用 (Burstable + 普通优先级)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: normal-app
spec:
  replicas: 5
  selector:
    matchLabels:
      app: normal
  template:
    metadata:
      labels:
        app: normal
    spec:
      priorityClassName: normal

      containers:
      - name: app
        image: normal-app:latest
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"  # Burstable QoS
            cpu: "500m"
---
# 批处理任务 (BestEffort + 低优先级)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: batch-job
spec:
  schedule: "0 2 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: batch
        spec:
          priorityClassName: low
          restartPolicy: OnFailure

          containers:
          - name: batch
            image: batch-processor:latest
            # 没有 resources - BestEffort QoS
```

### 案例 4: GPU 资源配额

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: gpu-quota
  namespace: ml-team
spec:
  hard:
    # GPU 资源
    requests.nvidia.com/gpu: "8"
    limits.nvidia.com/gpu: "8"

    # 计算资源
    requests.cpu: "40"
    requests.memory: "200Gi"
    limits.cpu: "80"
    limits.memory: "400Gi"

    # 对象数量
    pods: "50"
---
apiVersion: v1
kind: LimitRange
metadata:
  name: gpu-limits
  namespace: ml-team
spec:
  limits:
  - type: Container
    max:
      nvidia.com/gpu: "4"      # 单容器最多 4 个 GPU
      memory: "32Gi"
      cpu: "16"
    min:
      nvidia.com/gpu: "1"
      memory: "4Gi"
      cpu: "2"
    default:
      nvidia.com/gpu: "1"
      memory: "8Gi"
      cpu: "4"
```

## 最佳实践

### 1. 合理设置配额

```yaml
# ✅ 推荐: 根据实际需求设置
requests.cpu: "20"
requests.memory: "40Gi"

# ❌ 避免: 过大或过小
requests.cpu: "1000"    # 过大,浪费资源
requests.memory: "1Gi"  # 过小,限制过严
```

### 2. requests 和 limits 的比例

```yaml
# ✅ 推荐: 合理的比例
resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
  limits:
    memory: "512Mi"  # 2倍 requests
    cpu: "500m"      # 2倍 requests

# ❌ 避免: 过大的比例
resources:
  requests:
    memory: "100Mi"
    cpu: "100m"
  limits:
    memory: "10Gi"   # 100倍,可能OOM
    cpu: "10000m"    # 100倍,影响其他 Pod
```

### 3. 关键应用使用 Guaranteed

```yaml
# 关键应用
resources:
  requests:
    memory: "1Gi"
    cpu: "500m"
  limits:
    memory: "1Gi"    # 等于 requests
    cpu: "500m"      # 等于 requests
```

### 4. 设置默认值

```yaml
# 为命名空间设置 LimitRange
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: production
spec:
  limits:
  - type: Container
    default:
      memory: "512Mi"
      cpu: "500m"
    defaultRequest:
      memory: "256Mi"
      cpu: "250m"
```

### 5. 监控配额使用情况

```bash
# 查看配额使用
kubectl get resourcequota --all-namespaces

# 监控接近限制的命名空间
kubectl describe resourcequota -A | grep -A 5 "Used.*Hard"
```

## 常见问题

### Q1: Pod 创建失败,提示超过配额?

**错误信息:**
```
Error from server (Forbidden): pods "my-pod" is forbidden:
exceeded quota: compute-quota, requested: requests.cpu=2,requests.memory=4Gi,
used: requests.cpu=9,requests.memory=18Gi,
limited: requests.cpu=10,requests.memory=20Gi
```

**解决方法:**
```bash
# 1. 查看当前配额使用
kubectl describe resourcequota compute-quota -n production

# 2. 减少现有 Pod
kubectl scale deployment app --replicas=3 -n production

# 3. 或增加配额
kubectl edit resourcequota compute-quota -n production

# 4. 或优化资源请求
kubectl set resources deployment app --requests=cpu=500m,memory=512Mi
```

### Q2: 如何查看 Pod 为什么被驱逐?

```bash
# 查看 Pod 事件
kubectl describe pod <pod-name>

# 查看节点资源压力
kubectl describe node <node-name> | grep -A 10 Conditions

# 查看 QoS 类
kubectl get pod <pod-name> -o jsonpath='{.status.qosClass}'
```

### Q3: LimitRange 的默认值不生效?

**原因:**
- Pod 已经明确指定了 resources
- LimitRange 创建晚于 Pod
- LimitRange 配置错误

**检查:**
```bash
# 查看 LimitRange
kubectl describe limitrange -n production

# 删除并重建 Pod
kubectl delete pod my-pod -n production
kubectl apply -f pod.yaml
```

### Q4: 如何为现有 Pod 设置优先级?

```bash
# 方法1: 更新 Deployment
kubectl set priorityclass deployment/myapp critical

# 方法2: 编辑 YAML
kubectl edit deployment myapp
# 添加: spec.template.spec.priorityClassName: critical

# 方法3: Patch
kubectl patch deployment myapp -p '{"spec":{"template":{"spec":{"priorityClassName":"critical"}}}}'
```

### Q5: QoS 类可以手动设置吗?

不能。QoS 类由 Kubernetes 根据 resources 配置自动确定:

```yaml
# Guaranteed: requests == limits
resources:
  requests: {memory: 1Gi, cpu: 500m}
  limits:   {memory: 1Gi, cpu: 500m}

# Burstable: 有 requests 或 limits,但不等
resources:
  requests: {memory: 512Mi, cpu: 250m}
  limits:   {memory: 1Gi, cpu: 500m}

# BestEffort: 无 requests 和 limits
# (不设置 resources)
```

## 总结

- ✅ 理解 ResourceQuota 和 LimitRange 的作用
- ✅ 掌握 QoS 类的分类和优先级
- ✅ 熟悉优先级和抢占机制
- ✅ 能够实现多租户资源隔离
- ✅ 了解资源配额的最佳实践

## 参考资源

- [ResourceQuota 官方文档](https://kubernetes.io/zh-cn/docs/concepts/policy/resource-quotas/)
- [LimitRange 官方文档](https://kubernetes.io/zh-cn/docs/concepts/policy/limit-range/)
- [QoS 类](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/quality-service-pod/)
- [Pod 优先级和抢占](https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/)

---

*更新日期: 2025-12-03*
