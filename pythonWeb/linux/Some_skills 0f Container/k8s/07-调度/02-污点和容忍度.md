# 污点和容忍度

> Taints、Tolerations、节点专用化、驱逐策略

## 概述

【本文档是 Kubernetes 知识体系的一部分】

污点(Taint)和容忍度(Toleration)是 Kubernetes 调度的核心机制,用于控制 Pod 在节点上的调度和驱逐。污点使节点能够排斥某些 Pod,而容忍度则允许 Pod 容忍节点的污点。

## 核心概念

### 什么是污点和容忍度

**污点(Taint):**
- 应用于节点,使节点能够排斥某些 Pod
- 格式: `key=value:effect`
- 阻止不能容忍该污点的 Pod 调度到该节点

**容忍度(Toleration):**
- 应用于 Pod,使 Pod 能够容忍节点的污点
- 允许(但不要求)Pod 调度到有匹配污点的节点

### 工作原理

```
           节点有污点
                ↓
    ┌───────────────────────┐
    │  Node: master-node    │
    │  Taint:               │
    │  node-role=master:    │
    │  NoSchedule          │
    └───────────────────────┘
                ↓
    Pod 需要容忍度才能调度
                ↓
    ┌───────────────────────┐
    │  Pod: system-daemon   │
    │  Toleration:          │
    │  key: node-role       │
    │  operator: Exists     │
    │  effect: NoSchedule   │
    └───────────────────────┘
```

### 为什么需要

在生产环境中,我们需要污点和容忍度来实现:

1. **节点专用化**: 专用节点用于特定工作负载
2. **硬件隔离**: GPU 节点、高内存节点专用
3. **节点维护**: 标记节点为维护状态,驱逐 Pod
4. **问题节点隔离**: 硬件故障、网络问题等
5. **控制平面保护**: 防止普通 Pod 调度到 master 节点

### 污点效果 (Effect)

| Effect | 行为 | 说明 |
|--------|------|------|
| NoSchedule | 不调度 | 新 Pod 不会调度到该节点,已有 Pod 不受影响 |
| PreferNoSchedule | 尽量不调度 | 软约束,尽量避免但不保证 |
| NoExecute | 不调度+驱逐 | 新 Pod 不调度,已有 Pod 会被驱逐 |

## 基本使用

### 添加污点

```bash
# 基本格式
kubectl taint nodes <node-name> <key>=<value>:<effect>

# NoSchedule: 不调度新 Pod
kubectl taint nodes node1 dedicated=gpu:NoSchedule

# PreferNoSchedule: 尽量不调度
kubectl taint nodes node2 disktype=ssd:PreferNoSchedule

# NoExecute: 不调度并驱逐现有 Pod
kubectl taint nodes node3 status=maintenance:NoExecute
```

### 查看污点

```bash
# 查看所有节点的污点
kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints

# 查看特定节点的污点
kubectl describe node node1 | grep Taints

# 详细查看
kubectl get node node1 -o jsonpath='{.spec.taints}'
```

### 删除污点

```bash
# 删除特定污点
kubectl taint nodes node1 dedicated=gpu:NoSchedule-

# 删除所有同名污点(不管 value 和 effect)
kubectl taint nodes node1 dedicated-

# 删除节点的所有污点
kubectl patch node node1 -p '{"spec":{"taints":[]}}'
```

### Pod 容忍度

**完全匹配:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"

  containers:
  - name: cuda-app
    image: nvidia/cuda:11.0-base
```

**存在性匹配:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tolerant-pod
spec:
  tolerations:
  - key: "dedicated"
    operator: "Exists"  # 只要有该键就容忍
    effect: "NoSchedule"

  containers:
  - name: app
    image: myapp:latest
```

**容忍所有污点:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: super-tolerant-pod
spec:
  tolerations:
  - operator: "Exists"  # 容忍所有污点

  containers:
  - name: app
    image: system-daemon:latest
```

## 污点效果详解

### NoSchedule (不调度)

最常用的效果,阻止新 Pod 调度。

```bash
# 给节点添加 NoSchedule 污点
kubectl taint nodes node1 env=staging:NoSchedule
```

部署 Pod:
```yaml
# 没有容忍度的 Pod - 不会调度到 node1
apiVersion: v1
kind: Pod
metadata:
  name: no-toleration-pod
spec:
  containers:
  - name: nginx
    image: nginx:1.21
---
# 有容忍度的 Pod - 可以调度到 node1
apiVersion: v1
kind: Pod
metadata:
  name: with-toleration-pod
spec:
  tolerations:
  - key: "env"
    operator: "Equal"
    value: "staging"
    effect: "NoSchedule"

  containers:
  - name: nginx
    image: nginx:1.21
```

### PreferNoSchedule (尽量不调度)

软约束,调度器会尽量避免,但不保证。

```bash
# 添加 PreferNoSchedule 污点
kubectl taint nodes node2 disktype=slow:PreferNoSchedule
```

行为:
```
1. 调度器首先尝试其他节点
2. 如果其他节点资源不足,可能会调度到该节点
3. 已有 Pod 不受影响
```

使用场景:
- 节点性能较差,但可以作为备用
- 节点即将维护,希望减少新 Pod
- 测试环境节点,生产 Pod 尽量避免

### NoExecute (不调度+驱逐)

最严格的效果,会驱逐已有的不能容忍的 Pod。

```bash
# 添加 NoExecute 污点
kubectl taint nodes node3 status=draining:NoExecute
```

**立即驱逐:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: will-be-evicted
spec:
  # 没有容忍度,会被立即驱逐
  containers:
  - name: nginx
    image: nginx:1.21
```

**延迟驱逐:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tolerate-300s
spec:
  tolerations:
  - key: "status"
    operator: "Equal"
    value: "draining"
    effect: "NoExecute"
    tolerationSeconds: 300  # 容忍 300 秒后驱逐

  containers:
  - name: nginx
    image: nginx:1.21
```

**永久容忍:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: stay-forever
spec:
  tolerations:
  - key: "status"
    operator: "Equal"
    value: "draining"
    effect: "NoExecute"
    # 不设置 tolerationSeconds,永久容忍

  containers:
  - name: system-daemon
    image: daemon:latest
```

## 容忍度操作符

### Equal (相等)

精确匹配键、值和效果。

```yaml
tolerations:
- key: "dedicated"
  operator: "Equal"
  value: "gpu"
  effect: "NoSchedule"

# 匹配: dedicated=gpu:NoSchedule
# 不匹配: dedicated=cpu:NoSchedule
# 不匹配: dedicated=gpu:NoExecute
```

### Exists (存在)

只要键存在就匹配,忽略值。

```yaml
# 容忍特定键的所有值和效果
tolerations:
- key: "dedicated"
  operator: "Exists"

# 匹配:
# - dedicated=gpu:NoSchedule
# - dedicated=cpu:NoSchedule
# - dedicated=anything:NoExecute
```

**容忍所有污点:**
```yaml
tolerations:
- operator: "Exists"  # 不指定 key,匹配所有污点
```

## 实战案例

### 案例 1: Master 节点污点

默认情况下,master 节点有污点防止普通 Pod 调度。

```bash
# 查看 master 节点污点
kubectl describe node master-node | grep Taints

# 输出:
# Taints: node-role.kubernetes.io/control-plane:NoSchedule
#         node-role.kubernetes.io/master:NoSchedule
```

部署系统组件到 master:
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter

  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      # 容忍 master 节点污点
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        effect: NoSchedule

      hostNetwork: true
      hostPID: true

      containers:
      - name: node-exporter
        image: prom/node-exporter:v1.5.0
        args:
        - --path.procfs=/host/proc
        - --path.sysfs=/host/sys

        ports:
        - containerPort: 9100
          hostPort: 9100

        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true

      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
```

### 案例 2: GPU 节点专用

```bash
# 给 GPU 节点添加污点
kubectl taint nodes gpu-node-1 hardware=gpu:NoSchedule
kubectl taint nodes gpu-node-2 hardware=gpu:NoSchedule

# 验证
kubectl describe nodes gpu-node-1 | grep Taints
```

**普通 Pod(无法调度到 GPU 节点):**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: regular-pod
spec:
  containers:
  - name: app
    image: myapp:latest
    # 这个 Pod 不会调度到 GPU 节点
```

**GPU 工作负载(可以调度到 GPU 节点):**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-training
spec:
  tolerations:
  - key: "hardware"
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"

  containers:
  - name: training
    image: tensorflow/tensorflow:latest-gpu
    resources:
      limits:
        nvidia.com/gpu: 1

    command: ["python", "train.py"]
```

**Deployment 使用 GPU:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-training
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-training

  template:
    metadata:
      labels:
        app: ml-training
    spec:
      tolerations:
      - key: "hardware"
        operator: "Equal"
        value: "gpu"
        effect: "NoSchedule"

      # 配合 nodeSelector 确保调度到 GPU 节点
      nodeSelector:
        hardware: gpu

      containers:
      - name: training
        image: pytorch/pytorch:latest
        resources:
          limits:
            nvidia.com/gpu: 1
```

### 案例 3: 节点维护(驱逐 Pod)

```bash
# 标记节点为维护状态
kubectl taint nodes node1 status=maintenance:NoExecute

# Pod 会被立即驱逐,并在其他节点重建
# 等待所有 Pod 迁移完成
kubectl get pods -o wide | grep node1

# 维护完成后,删除污点
kubectl taint nodes node1 status=maintenance:NoExecute-
```

**关键服务配置延迟驱逐:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: critical

  template:
    metadata:
      labels:
        app: critical
    spec:
      tolerations:
      # 容忍维护状态 10 分钟
      - key: "status"
        operator: "Equal"
        value: "maintenance"
        effect: "NoExecute"
        tolerationSeconds: 600  # 10 分钟后驱逐

      containers:
      - name: app
        image: critical-app:latest
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"

        # 优雅关闭
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]

      terminationGracePeriodSeconds: 30
```

### 案例 4: 节点故障自动驱逐

Kubernetes 自动为问题节点添加污点:

```bash
# 查看节点条件
kubectl get nodes -o wide

# 节点 NotReady 时,会自动添加污点:
# node.kubernetes.io/not-ready:NoExecute
# node.kubernetes.io/unreachable:NoExecute
```

**默认容忍度:**
```yaml
# Kubernetes 自动为 Pod 添加的默认容忍度
tolerations:
- key: node.kubernetes.io/not-ready
  operator: Exists
  effect: NoExecute
  tolerationSeconds: 300  # 5 分钟

- key: node.kubernetes.io/unreachable
  operator: Exists
  effect: NoExecute
  tolerationSeconds: 300  # 5 分钟
```

**关键服务立即迁移:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fast-failover-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: fast-failover

  template:
    metadata:
      labels:
        app: fast-failover
    spec:
      tolerations:
      # 节点 NotReady 立即驱逐
      - key: node.kubernetes.io/not-ready
        operator: Exists
        effect: NoExecute
        tolerationSeconds: 10  # 10 秒后驱逐

      - key: node.kubernetes.io/unreachable
        operator: Exists
        effect: NoExecute
        tolerationSeconds: 10

      containers:
      - name: app
        image: fast-failover:latest
```

### 案例 5: 多租户隔离

```bash
# 为不同租户创建专用节点
kubectl taint nodes node1 node2 tenant=tenant-a:NoSchedule
kubectl taint nodes node3 node4 tenant=tenant-b:NoSchedule
kubectl taint nodes node5 node6 tenant=shared:PreferNoSchedule

# 查看污点分布
kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints
```

**租户 A 的应用:**
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-a
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tenant-a-app
  namespace: tenant-a
spec:
  replicas: 5
  selector:
    matchLabels:
      app: tenant-a-app

  template:
    metadata:
      labels:
        app: tenant-a-app
    spec:
      tolerations:
      - key: "tenant"
        operator: "Equal"
        value: "tenant-a"
        effect: "NoSchedule"

      nodeSelector:
        tenant: tenant-a  # 确保只调度到租户 A 节点

      containers:
      - name: app
        image: tenant-a/app:latest
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
```

**租户 B 的应用:**
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-b
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tenant-b-app
  namespace: tenant-b
spec:
  replicas: 3
  selector:
    matchLabels:
      app: tenant-b-app

  template:
    metadata:
      labels:
        app: tenant-b-app
    spec:
      tolerations:
      - key: "tenant"
        operator: "Equal"
        value: "tenant-b"
        effect: "NoSchedule"

      nodeSelector:
        tenant: tenant-b

      containers:
      - name: app
        image: tenant-b/app:latest
```

**共享服务(可以调度到任何节点):**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shared-service
spec:
  replicas: 2
  selector:
    matchLabels:
      app: shared

  template:
    metadata:
      labels:
        app: shared
    spec:
      tolerations:
      - operator: "Exists"  # 容忍所有污点

      containers:
      - name: app
        image: shared-service:latest
```

## 最佳实践

### 1. 污点命名规范

```bash
# 推荐: 使用有意义的键
kubectl taint nodes node1 dedicated=gpu:NoSchedule
kubectl taint nodes node1 env=production:NoSchedule
kubectl taint nodes node1 status=maintenance:NoExecute

# 避免: 使用模糊的键
# kubectl taint nodes node1 special=true:NoSchedule
# kubectl taint nodes node1 type1=yes:NoSchedule
```

### 2. 结合 nodeSelector 使用

```yaml
spec:
  # 1. nodeSelector: 限定节点范围
  nodeSelector:
    hardware: gpu

  # 2. toleration: 容忍节点污点
  tolerations:
  - key: "hardware"
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"
```

这样可以确保:
- Pod 只调度到 GPU 节点(nodeSelector)
- Pod 能够容忍 GPU 节点的污点(toleration)

### 3. 设置合理的 tolerationSeconds

```yaml
tolerations:
# 关键服务: 立即迁移
- key: node.kubernetes.io/not-ready
  operator: Exists
  effect: NoExecute
  tolerationSeconds: 10

# 普通服务: 等待 5 分钟
- key: node.kubernetes.io/not-ready
  operator: Exists
  effect: NoExecute
  tolerationSeconds: 300

# 状态服务: 永久容忍
- key: node.kubernetes.io/not-ready
  operator: Exists
  effect: NoExecute
  # 不设置 tolerationSeconds
```

### 4. 分层污点策略

```bash
# 层级 1: 硬件类型 (NoSchedule)
kubectl taint nodes gpu-node hardware=gpu:NoSchedule

# 层级 2: 环境 (PreferNoSchedule)
kubectl taint nodes gpu-node env=production:PreferNoSchedule

# 层级 3: 状态 (NoExecute)
kubectl taint nodes gpu-node status=maintenance:NoExecute
```

### 5. 自动化污点管理

```bash
# 脚本: 批量添加污点
for node in $(kubectl get nodes -l role=gpu -o name); do
  kubectl taint $node hardware=gpu:NoSchedule
done

# 脚本: 节点维护前添加污点
#!/bin/bash
NODE=$1
echo "Tainting node $NODE for maintenance..."
kubectl taint nodes $NODE status=maintenance:NoExecute
echo "Waiting for pods to evacuate..."
kubectl wait --for=delete pod --field-selector spec.nodeName=$NODE --timeout=600s
echo "Node ready for maintenance"
```

## 常见问题

### Q1: Pod 一直 Pending,提示污点不容忍?

**问题:**
```
Events:
  Warning  FailedScheduling  0/3 nodes are available: 1 node(s) had taint {dedicated: gpu}, that the pod didn't tolerate.
```

**排查步骤:**
```bash
# 1. 查看节点污点
kubectl describe nodes | grep -A 3 Taints

# 2. 查看 Pod 的容忍度
kubectl get pod <pod-name> -o yaml | grep -A 10 tolerations

# 3. 检查是否匹配
```

**解决方法:**
```yaml
# 添加相应的容忍度
tolerations:
- key: "dedicated"
  operator: "Equal"
  value: "gpu"
  effect: "NoSchedule"
```

### Q2: 如何临时允许 Pod 调度到 master?

```bash
# 方法1: 删除 master 污点(不推荐)
kubectl taint nodes master node-role.kubernetes.io/control-plane:NoSchedule-

# 方法2: Pod 添加容忍度(推荐)
kubectl run test --image=nginx --overrides='
{
  "spec": {
    "tolerations": [{
      "key": "node-role.kubernetes.io/control-plane",
      "effect": "NoSchedule"
    }]
  }
}'
```

### Q3: NoExecute 会驱逐系统 Pod 吗?

不会,系统 Pod(DaemonSet、Static Pod)通常有特殊的容忍度:

```yaml
tolerations:
- operator: "Exists"  # 容忍所有污点
  effect: "NoExecute"
```

### Q4: 如何排查 Pod 被驱逐?

```bash
# 查看 Pod 事件
kubectl describe pod <pod-name> | grep -A 10 Events

# 查看节点污点变化
kubectl get events --field-selector involvedObject.name=<node-name>

# 查看 Pod 终止原因
kubectl get pod <pod-name> -o jsonpath='{.status.reason}'
```

### Q5: 污点和容忍度 vs nodeSelector 如何选择?

**使用污点/容忍度:**
- ✅ 阻止 Pod 调度(排斥逻辑)
- ✅ 专用节点、节点维护
- ✅ 驱逐 Pod
- ✅ 软约束(PreferNoSchedule)

**使用 nodeSelector:**
- ✅ 选择特定节点(吸引逻辑)
- ✅ 简单的标签匹配
- ✅ 硬性要求

**结合使用:**
```yaml
spec:
  nodeSelector:
    hardware: gpu  # 选择 GPU 节点
  tolerations:
  - key: hardware
    value: gpu
    effect: NoSchedule  # 容忍 GPU 节点的污点
```

## 总结

- ✅ 理解污点和容忍度的工作原理
- ✅ 掌握三种效果(NoSchedule、PreferNoSchedule、NoExecute)
- ✅ 熟悉容忍度操作符(Equal、Exists)
- ✅ 能够实现节点专用化和故障处理
- ✅ 了解污点管理最佳实践

## 参考资源

- [污点和容忍度官方文档](https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/)
- [节点维护](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/safely-drain-node/)
- [节点条件](https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/#condition)

---

*更新日期: 2025-12-03*
