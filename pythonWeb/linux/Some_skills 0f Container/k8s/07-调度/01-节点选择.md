# 节点选择

> nodeSelector、nodeName、节点标签管理

## 概述

【本文档是 Kubernetes 知识体系的一部分】

节点选择允许你控制 Pod 被调度到哪些节点上。通过节点标签、节点选择器等机制,可以将 Pod 调度到满足特定条件的节点。

## 核心概念

### 什么是节点选择

节点选择是 Kubernetes 调度器根据节点标签和 Pod 的节点选择器,将 Pod 调度到合适节点的机制。

**核心特性:**
- **标签匹配**: 基于节点标签进行选择
- **硬性要求**: nodeSelector 和 nodeName 是强制性的
- **简单直观**: 最简单的节点约束方式
- **灵活扩展**: 可自定义任意标签

### 为什么需要

在生产环境中,我们需要节点选择来实现:

1. **硬件隔离**: GPU 节点专用于机器学习任务
2. **存储分离**: SSD 节点用于高 I/O 应用
3. **环境隔离**: 开发/测试/生产环境分离
4. **区域分布**: 跨可用区部署
5. **专用节点**: 特殊工作负载独占节点

### 节点选择方式对比

| 方式 | 类型 | 灵活性 | 使用场景 |
|------|------|--------|---------|
| nodeName | 硬性约束 | 低 | 测试、调试 |
| nodeSelector | 硬性约束 | 中 | 简单标签匹配 |
| nodeAffinity | 软/硬约束 | 高 | 复杂调度需求 |

## 基本使用

### nodeSelector (节点选择器)

最简单的节点选择方式,通过标签匹配节点。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-ssd
spec:
  nodeSelector:
    disktype: ssd  # 匹配带有此标签的节点
  containers:
  - name: nginx
    image: nginx:1.21
```

**工作原理:**
```
1. 调度器检查 Pod 的 nodeSelector
2. 过滤掉不包含匹配标签的节点
3. 从剩余节点中选择最佳节点
4. 如果没有匹配节点,Pod 保持 Pending 状态
```

### 节点标签管理

```bash
# 给节点添加标签
kubectl label nodes node1 disktype=ssd
kubectl label nodes node1 env=production
kubectl label nodes node1 zone=us-east-1a

# 查看节点标签
kubectl get nodes --show-labels

# 查看特定节点的标签
kubectl describe node node1 | grep Labels -A 10

# 更新标签
kubectl label nodes node1 disktype=nvme --overwrite

# 删除标签
kubectl label nodes node1 disktype-

# 根据标签选择节点
kubectl get nodes -l disktype=ssd
kubectl get nodes -l env=production,zone=us-east-1a
```

### 多标签匹配

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-workload
spec:
  nodeSelector:
    accelerator: nvidia-tesla-v100
    disktype: ssd
    zone: us-east-1a
  containers:
  - name: ml-app
    image: tensorflow/tensorflow:latest-gpu
    resources:
      limits:
        nvidia.com/gpu: 1
```

**匹配规则:**
- 节点必须同时具有所有指定标签
- 标签的键和值都必须完全匹配
- 是 AND 关系,不是 OR 关系

### nodeName (直接指定节点)

跳过调度器,直接指定 Pod 运行的节点。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-node1
spec:
  nodeName: node1  # 直接指定节点名称
  containers:
  - name: nginx
    image: nginx:1.21
```

**特点:**
- **优先级最高**: 跳过所有调度逻辑
- **无验证**: 不检查节点是否存在或可用
- **不推荐**: 生产环境应避免使用
- **调试用途**: 适合测试和故障排查

### 内置节点标签

Kubernetes 自动为节点添加的标签:

```yaml
# 主机名
kubernetes.io/hostname: node1

# 操作系统
kubernetes.io/os: linux
beta.kubernetes.io/os: linux

# 架构
kubernetes.io/arch: amd64
beta.kubernetes.io/arch: amd64

# 实例类型 (云环境)
node.kubernetes.io/instance-type: m5.large

# 可用区 (云环境)
topology.kubernetes.io/zone: us-east-1a
topology.kubernetes.io/region: us-east-1
failure-domain.beta.kubernetes.io/zone: us-east-1a
failure-domain.beta.kubernetes.io/region: us-east-1
```

使用内置标签:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: linux-only-pod
spec:
  nodeSelector:
    kubernetes.io/os: linux
    kubernetes.io/arch: amd64
  containers:
  - name: app
    image: myapp:latest
```

## Deployment 中使用

### 基础示例

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      nodeSelector:
        env: production
        tier: frontend
      containers:
      - name: nginx
        image: nginx:1.21
        resources:
          requests:
            memory: "256Mi"
            cpu: "500m"
```

### 跨区域部署

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-zone-app
spec:
  replicas: 6
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      # 使用 podAntiAffinity 结合 nodeSelector
      nodeSelector:
        env: production

      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: myapp
              topologyKey: topology.kubernetes.io/zone

      containers:
      - name: app
        image: myapp:latest
```

## 实战案例

### 案例 1: GPU 节点专用

```bash
# 给 GPU 节点打标签
kubectl label nodes gpu-node-1 accelerator=nvidia-tesla-v100
kubectl label nodes gpu-node-2 accelerator=nvidia-tesla-v100

# 验证标签
kubectl get nodes -l accelerator=nvidia-tesla-v100
```

部署 GPU 工作负载:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-training
  labels:
    app: ml-training
spec:
  nodeSelector:
    accelerator: nvidia-tesla-v100

  containers:
  - name: training
    image: pytorch/pytorch:latest
    command: ["python", "train.py"]

    resources:
      limits:
        nvidia.com/gpu: 1

    volumeMounts:
    - name: dataset
      mountPath: /data

  volumes:
  - name: dataset
    persistentVolumeClaim:
      claimName: training-data-pvc
```

### 案例 2: SSD 存储节点

```bash
# 标记 SSD 节点
kubectl label nodes node1 disktype=ssd
kubectl label nodes node2 disktype=ssd
kubectl label nodes node3 disktype=hdd

# 查看存储类型分布
kubectl get nodes -L disktype
```

部署数据库到 SSD 节点:
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql

  template:
    metadata:
      labels:
        app: mysql
    spec:
      nodeSelector:
        disktype: ssd  # 只调度到 SSD 节点

      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password

        ports:
        - containerPort: 3306
          name: mysql

        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql

        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"

  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 20Gi
```

### 案例 3: 环境隔离

```bash
# 给节点打环境标签
kubectl label nodes node1 node2 node3 env=production
kubectl label nodes node4 node5 env=staging
kubectl label nodes node6 env=development

# 查看环境分布
kubectl get nodes -L env
```

**生产环境部署:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-production
  namespace: production
spec:
  replicas: 5
  selector:
    matchLabels:
      app: myapp
      env: prod

  template:
    metadata:
      labels:
        app: myapp
        env: prod
    spec:
      nodeSelector:
        env: production  # 只部署到生产节点

      containers:
      - name: app
        image: myapp:v1.2.3
        env:
        - name: ENVIRONMENT
          value: "production"

        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"

        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10

        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
```

**开发环境部署:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-development
  namespace: development
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
      env: dev

  template:
    metadata:
      labels:
        app: myapp
        env: dev
    spec:
      nodeSelector:
        env: development  # 只部署到开发节点

      containers:
      - name: app
        image: myapp:dev-latest
        env:
        - name: ENVIRONMENT
          value: "development"

        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
```

### 案例 4: 多租户隔离

```bash
# 为不同租户创建专用节点池
kubectl label nodes node1 node2 tenant=tenant-a
kubectl label nodes node3 node4 tenant=tenant-b
kubectl label nodes node5 node6 tenant=shared

# 查看租户分布
kubectl get nodes -L tenant
```

**租户 A 的应用:**
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-a
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tenant-a-app
  namespace: tenant-a
spec:
  replicas: 3
  selector:
    matchLabels:
      app: tenant-a-app

  template:
    metadata:
      labels:
        app: tenant-a-app
    spec:
      nodeSelector:
        tenant: tenant-a  # 专用节点

      containers:
      - name: app
        image: tenant-a/app:latest
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
```

**租户 B 的应用:**
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-b
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tenant-b-app
  namespace: tenant-b
spec:
  replicas: 2
  selector:
    matchLabels:
      app: tenant-b-app

  template:
    metadata:
      labels:
        app: tenant-b-app
    spec:
      nodeSelector:
        tenant: tenant-b  # 不同的专用节点

      containers:
      - name: app
        image: tenant-b/app:latest
```

## 最佳实践

### 1. 标签命名规范

```bash
# 推荐: 使用有意义的前缀
kubectl label nodes node1 company.com/disktype=ssd
kubectl label nodes node1 company.com/env=production
kubectl label nodes node1 company.com/zone=us-east-1a

# 避免: 使用模糊的标签
# kubectl label nodes node1 type=1
# kubectl label nodes node1 env=prod1
```

**命名建议:**
- 使用域名前缀避免冲突
- 使用有意义的键名
- 值应该简洁明确
- 避免使用特殊字符

### 2. 标签管理策略

```yaml
# 在 Node 创建时就打好标签
apiVersion: v1
kind: Node
metadata:
  name: node1
  labels:
    # 硬件特性
    disktype: ssd
    gpu: nvidia-tesla-v100
    memory-size: large

    # 位置信息
    zone: us-east-1a
    region: us-east-1

    # 用途分类
    env: production
    tier: compute
    workload-type: ml
```

### 3. 避免使用 nodeName

```yaml
# ❌ 不推荐: 硬编码节点名
spec:
  nodeName: node1

# ✅ 推荐: 使用 nodeSelector
spec:
  nodeSelector:
    kubernetes.io/hostname: node1
```

### 4. 结合其他调度特性

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: critical

  template:
    metadata:
      labels:
        app: critical
    spec:
      # 1. nodeSelector: 硬性要求
      nodeSelector:
        disktype: ssd
        env: production

      # 2. affinity: 软性偏好
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: zone
                operator: In
                values:
                - us-east-1a

        # 3. podAntiAffinity: Pod 分散
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: critical
            topologyKey: kubernetes.io/hostname

      # 4. tolerations: 容忍污点
      tolerations:
      - key: dedicated
        operator: Equal
        value: critical-apps
        effect: NoSchedule

      containers:
      - name: app
        image: critical-app:latest
```

### 5. 动态标签管理

```bash
# 批量添加标签
kubectl label nodes -l region=us-east-1 zone=us-east-1a

# 条件更新
kubectl label nodes node1 disktype=nvme --overwrite

# 批量删除标签
kubectl label nodes -l env=old env-
```

## 常见问题

### Q1: Pod 一直 Pending,提示节点选择器不匹配?

**问题:**
```
Events:
  Warning  FailedScheduling  0/3 nodes are available: 3 node(s) didn't match node selector.
```

**排查步骤:**
```bash
# 1. 查看 Pod 的 nodeSelector
kubectl get pod <pod-name> -o yaml | grep -A 5 nodeSelector

# 2. 查看所有节点的标签
kubectl get nodes --show-labels

# 3. 查找匹配的节点
kubectl get nodes -l disktype=ssd

# 4. 检查节点是否可调度
kubectl describe nodes | grep Taints
```

**解决方法:**
```bash
# 方法1: 给节点添加所需标签
kubectl label nodes node1 disktype=ssd

# 方法2: 修改 Pod 的 nodeSelector
kubectl edit pod <pod-name>

# 方法3: 删除 nodeSelector
kubectl patch pod <pod-name> -p '{"spec":{"nodeSelector":null}}'
```

### Q2: 如何查看哪些 Pod 运行在特定节点?

```bash
# 查看节点上的所有 Pod
kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=node1

# 查看使用特定标签选择器的 Pod
kubectl get pods --all-namespaces -o json | \
  jq -r '.items[] | select(.spec.nodeSelector.disktype=="ssd") | .metadata.name'

# 统计每个节点的 Pod 数量
kubectl get pods --all-namespaces -o json | \
  jq -r '.items[].spec.nodeName' | sort | uniq -c
```

### Q3: nodeSelector 和 nodeAffinity 如何选择?

**使用 nodeSelector:**
- ✅ 简单的标签匹配
- ✅ 易于理解和维护
- ✅ 硬性要求

**使用 nodeAffinity:**
- ✅ 复杂的匹配逻辑 (In, NotIn, Exists, DoesNotExist)
- ✅ 支持软性偏好 (preferred)
- ✅ 支持多个条件组合

示例对比:
```yaml
# nodeSelector (简单)
nodeSelector:
  disktype: ssd

# nodeAffinity (强大)
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: disktype
          operator: In
          values:
          - ssd
          - nvme
```

### Q4: 如何批量迁移 Pod 到新节点?

```bash
# 1. 给新节点打标签
kubectl label nodes new-node1 migration=true

# 2. 驱逐旧节点的 Pod (会在新节点重建)
kubectl drain old-node1 --ignore-daemonsets --delete-emptydir-data

# 3. 验证 Pod 调度到新节点
kubectl get pods -o wide

# 4. 清理旧节点
kubectl delete node old-node1
```

### Q5: 节点标签最多可以有多少个?

Kubernetes 对标签没有硬性数量限制,但有以下建议:
- 标签总大小不超过 63 字符(键 + 值)
- 合理数量通常在 10-20 个
- 过多标签会影响性能

## 总结

- ✅ 理解 nodeSelector 和 nodeName 的区别
- ✅ 掌握节点标签的添加、查询、删除
- ✅ 熟悉内置节点标签的使用
- ✅ 能够实现硬件隔离、环境隔离
- ✅ 了解标签命名和管理最佳实践

## 参考资源

- [节点选择器官方文档](https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/)
- [节点标签](https://kubernetes.io/zh-cn/docs/concepts/overview/working-with-objects/labels/)
- [Well-Known Labels](https://kubernetes.io/docs/reference/labels-annotations-taints/)

---

*更新日期: 2025-12-03*
